<div class="flex justify-center">
  <article class="my-2 prose w-full text-white">
    <h1 class="text-white">Chains of Thought and Action (CoTA)</h1>
    <p>
      We define a <i>chain of thought and action</i> as a series of reasoning
      steps and tool use whereby agentic AI systems show both the intermediate
      reasoning and the inputs and outputs of actions taken.
    </p>

    <h2 class="text-white">Chain of Thought</h2>
    <p>
      A 'chain of thought' is a series of intermediate reasoning steps that
      significantly improves the ability of large language models to perform
      complex reasoning. Introduced in
      <a
        href="https://arxiv.org/abs/2201.11903"
        class="text-white hover:opacity-75 underline"
        target="_blank"
        >Chain-of-Thought Prompting Elicits Reasoning in Large Language
        Models</a
      >, this technique demonstrated a 20-30% improvement in reasoning accuracy
      by having models explain their solutions step-by-step, rather than jumping
      directly to answers.
    </p>

    <p>
      OpenAI has embraced this concept with their O1 series of models, which are
      specifically designed to "spend more time thinking before they respond."
      However, OpenAI has been
      <a
        href="https://x.com/parshantdeep/status/1834281496850694544"
        class="text-white hover:opacity-75 underline"
        target="_blank"
        >criticized</a
      >
      for not showing the chain of thought to users, unlike competitors like
      DeepSeek which display the complete, unedited reasoning steps.
    </p>

    <h2 class="text-white">And Action</h2>
    <p>
      While chain of thought reveals an AI's reasoning process, we need to
      extend this transparency to actions as well. This section will demonstrate
      how we can make AI systems show not just their thinking, but also their
      tool use and actions in achieving tasks.
    </p>

    <h2 class="text-white">An Example</h2>
    <p>
      Imagine a 'GitHub issue solver' agent that submits a pull request with
      code that solves a given GitHub issue. It would combine multi-step
      reasoning and tool use to understand codebases and documentation, use CI
      tools and tests, and interact with the GitHub and other external APIs.
    </p>
    <p>
      Importantly, this agent would show the full trace of its CoTA: including
      both its reasoning (using a model like DeepSeek R1) and tool use
      (displaying input/output of each tool as sent to the LLM).
    </p>
    <p>The workflow combines multiple models and tools, perhaps:</p>

    <ul class="list-disc list-inside text-white">
      <li>Build repository map from issue context (automated script)</li>
      <li>
        Identify relevant files (DeepSeek R1 for reasoning, Mistral Small for
        structured output)
      </li>
      <li>Traverse and analyze codebase (file readers, AST parsers)</li>
      <li>Plan changes (DeepSeek R1 with full reasoning trace)</li>
      <li>Generate and test code changes (coding tools, CI integration)</li>
      <li>Create pull request with detailed explanation (GitHub API tools)</li>
    </ul>
    <p>
      At each step, both the reasoning process and tool interactions are logged
      and displayed, creating a complete audit trail of how the solution was
      developed.
    </p>
    <p>
      Entrusting such an agent to make sensitive edits to a codebase may be
      foolish if its creator were to refuse to show you its thought process!
    </p>
    <p>
      We will build the above 'issue solver' agent over the next few videos of
      our
      <a class="text-white hover:opacity-75 underline" href="/video-series"
        >series</a
      >.
    </p>
  </article>
</div>
