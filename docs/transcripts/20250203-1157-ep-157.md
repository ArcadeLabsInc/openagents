---
source_file: temp_audio.flac
timestamp: 2025-02-03T11:57:00.743362-06:00
model: whisper-large-v3
---

Welcome to the post-DeepSeek R1 era. Open source has caught up to the closed labs, and now we need to power past them. So we're introducing this concept called Chains of Thought and Action, combining the magic of reasoning models like DeepSeek R1 with that same concept applied to agents, the tool use. Okay, this tweet got me thinking. blah blah blah my central problem with large reasoning models like o3 and r1 is that they claim to check their answer without really checking them why do all your thinking in a single generation if you can split it in 10 using much smaller modules yeah so this concept of reasoning generating a chain of thought like sure it improves over just like a single shot a few shot generation but who's to say that that thinking should be limited only to the training data. Why not give it access to live tools? Reality checks. Yeah, like break this up into not just model calls, but also tool calls, you know, agents.

So OpenAI had a release today. Where did it go? I had a little viral tweet here. That's nice. So the 157 billion soon billion company is posting model leaderboards where the top model deep research model, this guy's from OpenAI, by the way. The top model uses tools like browsing compared against models that do not use tools. This is beyond meaningless. It's deceptive. Look at it. The only thing on here that uses tools. Like, that's not Apple's to Apple's comparison. This is shady. But what is deep research? It's a model, 03 or mini or something, with some fine-tuning, that they give access to tools. Okay, cool. This should have been compared against some of the very many awesome projects that are out there, many of them open source, doing agentic web crawling. But then they would have only probably demonstrated an incremental improvement.

So, you know, OpenAI not disclosing their reasoning traces. They've said specifically in a previous blog post that they are intentionally hiding the chains of thought. Some people got excited at their, the like Reddit AMA where Allman yesterday was like, oh, they're like, oh, he's going to show the chains of thought. No, they're gonna do better about maybe making, improving it like just mealy mouth bullshit So we in open source land here We gonna show our traces We not scared of whatever all this crap is Okay But we want to see those for both reasoning as well as tools

So I figured that, you know, even though we may do our own little kind of archive post at some point, like the Chain of Thought folks did, you know, I'm just some dumb app builder. So I'm to build a dumb little app, we're going to code this up. So here's the example. We're going to do a GitHub issue solver agent that submits a pull request with code that solves the given GitHub issue, combining multi-step reasoning and tool use to understand code bases and documentation, use CI tools and tests, and interact with the GitHub and other external APIs. I do a version of this every day. I do about 80% of my coding through the OpenAgents Pro dashboard. This is not really public-facing, just me and some kind of private clients. But we're doing a version of this that's going to be public-facing, released later this month. But just, you know, me saying stuff.

It loops through. It shows me the tools that it uses, input, output. And the next version of this is going to be a little bit more autonomous, a little bit more like Devin, but like full transparency for all these things. Okay, so this agent is also, and all of these agents should show, the full trace of their COTA, including both the reasoning, using a model like DeepSeq R1, and tool use displaying input output of each tool as sent to the LLM Or if a model that not a reasoning model if it sent for some reason to Claude because it better at some things still let get some transparency about what are the inputs and outputs there and let those be individually tested or inspectable or upgradable.

And so this workflow may combine various things like building a repo map from the issue context. Our previous video, I was doing a repo map from AIDER. We've since built our own version kind of modeled off of theirs internal to Rust. That's just a script. We're going to pass that script to some model or models. So one of the things I want to do in this next series of videos, I'll do a video on like building an eval. So we can test to see what model or combination of models can generate the best and most relevant files based on the repo map relevant to the issue. We're going to traverse the relevant files, analyze things, plan out the changes, generate the code, generate a PR, and then do some sort of test and CI. So some combination of a bunch of different things. And, yeah, let's just be transparent and show all this. Blah, blah, blah, yeah.

If you're entrusting such an agent to make sensitive edits to a code base, if you're hiring an AI engineer and they don't show you their code, they don't show you their trace, their thought process, what they're doing, Why would you trust that when you have an alternative called an open agent that does show you all that? All right, so we're going to build this issue solver agent over the next few videos of our series. Follow us on X if you haven't. It'll be fun. See you soon.
