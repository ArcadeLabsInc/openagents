# Real-Time Sync Engine Architectures: Linear vs. Figma

## Linear’s Local-First Sync Engine

Linear’s approach is local-first and offline-friendly. The Linear client (web or desktop) maintains a full local copy of most data (issues, projects, etc.) in an IndexedDB database in the browser ￼. On first load, the client performs a bootstrap sync by calling an API (e.g. /sync/bootstrap) to fetch all initial data. This bootstrap returns each model’s full data (as JSON) so the app can render the UI immediately ￼ ￼. After the initial load, all further communication with the server happens through a WebSocket-based sync protocol instead of traditional REST API calls.

Every data mutation (create/update/delete) in Linear is turned into a SyncAction that gets sent over the WebSocket. For example, creating a comment produces a SyncAction with an “Insert” for the new comment, and often an “Update” for related data (like an issue’s updated timestamp) ￼ ￼. Each SyncAction has a monotonically increasing ID and includes the type of operation (Insert, Update, Delete, etc.), the model name, and the new data or changed fields ￼ ￼. The client applies these incoming actions to its local database to keep the UI state in sync with the server. Because all clients receive the same ordered stream of actions, everyone’s local copy eventually converges to the same state.

Linear’s sync engine ensures real-time collaboration by pushing updates instantly to all connected clients. When you make a change in Linear, the app optimistically updates your local state and sends the mutation to the server (often via a GraphQL mutation that triggers the sync) ￼. The server then broadcasts the official SyncAction to all clients (including yours) over WebSocket. The use of a WebSocket channel replaces many traditional REST API calls – rather than clients polling for changes or receiving full updated records on each request, they get a steady stream of incremental updates. This design makes Linear feel extremely fast and “instantly” updated, since most interactions are applied locally first and confirmed via the sync channel shortly after.

To handle offline usage, Linear leverages the local cache and the sequential IDs of SyncActions. Each client tracks the last sync ID it has processed. If a client goes offline or is suspended and misses some updates, it can reconnect later and request a delta sync for any missed actions. The server exposes a /sync/delta endpoint where the client can specify the last ID it saw and get all subsequent SyncAction objects it missed ￼. The client then replays those actions to catch up its local state ￼. This ensures that after a period of disconnection, a client doesn’t need to re-download everything — it can just get the incremental changes (which is much more efficient). The lastSyncId is included in each WebSocket payload as well ￼, allowing the client to detect if there’s a gap. In practice, this mechanism keeps data consistent across clients: every change is numbered and ordered, so clients can apply them in order or request any missing range to converge on the correct state.

## Figma’s Multiplayer Sync Engine

Figma’s architecture is built for multiplayer, live collaboration on design files. Unlike Linear (which deals with discrete tasks and can easily use last-writer-wins), Figma needs to support multiple users editing the same canvas simultaneously with fine-grained operations. Figma’s solution was to design a homegrown sync system that avoids the complexity of fully decentralized CRDTs, instead leveraging a central authority (the server) to coordinate changes ￼. In Figma’s model, each document’s state is managed by a dedicated server process. All client actions (e.g. moving an object, editing text) are sent to that server process, which assigns a global order to every operation and then broadcasts the operation to all clients viewing the document ￼ ￼. This ensures that every client applies the exact same sequence of operations in the same order, achieving consistency without complex merge algorithms.

By having the server dictate a single ordering of events, Figma sidesteps many hard problems of conflict resolution. Evan Wallace (Figma’s co-founder) explained that “Figma isn’t using true CRDTs […]. CRDTs are designed for decentralized systems with no single authority… Since Figma is centralized (our server is the central authority), we can remove this extra overhead and benefit from a faster and leaner implementation.” ￼. In practice, this means the server can simply sequence edits and apply them one-by-one. All replicas (clients) receive every operation in the same order, so the document state remains in sync ￼. This approach also allows the server to enforce rules or constraints: if an operation would violate a consistency invariant (say, causing a corrupted state), the server can reject or adjust it before broadcasting ￼.

Figma’s sync engine is highly optimized for performance and low latency. The collaborative editing server is written in Rust for speed and to handle many concurrent updates with minimal lag ￼. Changes are transmitted as small diffs or operation messages rather than re-sending entire documents. For example, if a user moves an object or changes a property, only that operation is sent to others, often encoded in a compact format. This keeps bandwidth usage and latency low, essential for a real-time design tool. Figma initially required an active internet connection for collaboration, but it later introduced a limited offline mode. In offline mode, a user can continue making edits locally; once reconnected, the client will synchronize with the server. Because Figma’s server is the source of truth, the client must send all offline-made changes to the server and receive any concurrent edits from others. The server will merge these by integrating the offline operations into the global sequence. Essentially, the client performs a rebase: it fetches any missing operations from the server, then applies its own queued operations on top in the same order. Thanks to the centralized design, conflict resolution in Figma is simplified – the last operation in the global order “wins” for any particular element/property. In cases where two users truly conflict (e.g. both change the exact same text at once), one change will overwrite the other based on ordering, or the later one may be applied after the earlier, potentially undoing it. Figma relies on the fact that such exact conflicts are rare in practice (especially because the app shows cursors/presence, so users can see each other’s focus). Most of the time, each user is editing different objects or different parts of the canvas, so their operations don’t collide directly.

## Data Consistency and Conflict Resolution Strategies

Both Linear and Figma ensure eventual consistency across clients by delivering the same set of changes to all replicas, but they handle conflicts differently given their use cases. In Linear’s case, the domain (issue tracking) means true concurrent edits on the exact same field are uncommon. Linear’s sync engine doesn’t attempt elaborate merge algorithms; it effectively uses a last-writer-wins strategy for most updates. The server (backed by PostgreSQL) is the source of truth – whichever update arrives last will be reflected in the latest SyncAction and state. If two users edit the same issue title nearly simultaneously, the one that the server processes second will override the first. This is a conscious design choice, as simpler conflict resolution is usually sufficient for Linear’s use case. In fact, for many “business object” applications, the complexity of Operational Transformation (OT) or CRDT algorithms is overkill, and a “straightforward last-writer-wins approach might suffice” ￼. Linear’s team has noted that building a full OT/CRDT system for an issue tracker would add huge complexity for little gain, since most edits are quick and don’t truly collide ￼. They instead optimize for fast updates and simplicity, accepting that on the rare occasion a conflict occurs, the latest edit wins (and users can manually resolve any discrepancy, such as re-typing a lost description if needed).

Figma, on the other hand, initially appears to face a much harder consistency problem (multiple users editing a complex structured document). However, Figma avoids most conflicts by serializing all edits through the server. The central document server orders every change, so clients never end up with divergent histories that need merging – there is a single canonical sequence of operations. This means Figma’s consistency model is closer to strong consistency: at any given moment, all online collaborators see the same version of the document (modulo network latency). For conflict resolution, Figma’s philosophy is to prevent divergence rather than attempt to merge changes post-facto. If two operations do interfere (e.g. two people edit the same text box concurrently), one of them will be applied after the other in sequence. In effect, one user’s change might override the other’s, or the changes will both apply if they affect different parts of the text (depending on how the operations are defined). Figma does not try to merge two conflicting edits – it simply applies them in some order. This can occasionally lead to one collaborator’s change being lost if they truly edited the exact same property at the same time as someone else. But in practice this is extremely rare, and Figma (like many collaborative tools) relies on human collaboration dynamics to minimize it. Users usually avoid editing exactly the same thing simultaneously (“social locking”), especially since presence indicators make it clear what others are focused on ￼. If a conflict does happen, “in the rare times it does occur, the users can sort it out among themselves.” ￼ In other words, the app trusts the users to communicate or undo/redo as needed rather than implementing complex automatic merging. This approach keeps the system simpler and faster. A general theme in successful real-time collaboration systems is not over-engineering conflict resolution – a somewhat “naive” approach can work fine in practice, given the human-in-the-loop nature of collaboration ￼.

To summarize, data consistency in both systems is achieved by ensuring all clients process a globally ordered list of changes (Linear’s via incrementing IDs with possible catch-up, Figma’s via a server sequence). Conflict resolution is mostly handled by ordering (whoever’s change comes later takes effect). Neither Linear nor Figma uses heavy algorithmic merging like CRDTs in the fully decentralized sense. Instead, they leverage a central source of truth (a server or a transaction log) to make decisions, and they assume true simultaneous conflicts are infrequent. This yields a much simpler implementation while still delivering a real-time collaborative experience.

## Performance and Scalability Considerations

Both sync engines are designed with performance in mind, but they optimize different aspects. Linear’s sync engine prioritizes UI responsiveness and low latency for typical app interactions. By storing data locally on the client, Linear can render and update the UI immediately without waiting for server round-trips. Even when offline, a user can perform actions (which update the local cache) and see the result instantly, with the changes queued to sync when connectivity returns. This local-first approach makes the app feel snappy and reliable. Over the network, Linear sends only lightweight incremental updates (the SyncActions), which contain just the fields that changed for a given model. This keeps bandwidth usage minimal compared to sending full objects or making frequent REST calls. Linear also batches related changes together. For example, if you change an issue’s title and add a comment, the server might send two SyncAction objects in one WebSocket message (one update to the issue, one insert for the comment) ￼ ￼. Batching multiple operations in a single message reduces overhead from network frames and ensures that clients apply a set of related changes together.

On the server side, Linear uses PostgreSQL to store data and the ordered log of sync actions. The sync engine scales horizontally by treating each workspace/organization separately – a given user only cares about their organization’s data. This means the server can send updates only to the clients in the relevant org or team, avoiding broadcasting every change to every connected client. As the number of models and data grew, Linear introduced optimizations like partial bootstraps (only syncing critical models first, and deferring less important data) ￼ ￼ to reduce initial load time. Scaling challenges that Linear encountered include handling a large volume of sync actions as the dataset grows, and ensuring that catching up via delta remains efficient. Using an indexed incremental ID (the lastSyncId) helps here – the server can quickly query “all actions since ID X” for a client, which is much faster than comparing full dataset snapshots. In practice, Linear might periodically compact or trim the sync log (since very old actions become less relevant once all clients have long moved past them), though details of that aren’t public. The design choice to use a simple integer ID and sequential log is easy to maintain and can be sharded or partitioned by organization to scale. Linear’s team has discussed how the sync API evolved as they scaled, indicating they continually tuned performance (e.g., ensuring the WebSocket can handle bursts of updates, optimizing how data is cached on client, etc.) ￼ ￼.

Figma’s performance focus is on real-time concurrency at possibly high frequencies (many cursor moves, object transforms per second) and consistency for all collaborators. Figma keeps the authoritative document state in memory on the server (within that per-document process). This allows extremely fast application of operations and generation of diffs to send to clients. The use of a high-performance language (Rust) on the server means it can handle a high throughput of operations and manage memory efficiently ￼. Figma’s network protocol is likely binary or highly compressed JSON, sending just the essential data for each change. Because Figma documents can be large (with many vector objects, images, etc.), the sync engine must avoid re-sending large portions of the file. Instead, it sends atomic operations like “insert node X as child of Y” or “change property Z of object #123 to value V”. This ensures bandwidth usage grows roughly with the number of edits, not the size of the document. Figma also uses intelligent strategies to manage undo/redo and history on the client, so that applying incoming operations doesn’t blow away your undo stack and vice versa.

For scalability, Figma’s decision to have one dedicated server process per document means they can handle many documents in parallel by spinning up multiple processes (or threads). Each document collaboration session is essentially its own “room.” This isolates the load: 10 large collaborative files open at once will use 10 separate processes, preventing one heavy file from blocking others ￼. The downside is that if a document has hundreds of concurrent users, the single process handling it must manage that fan-out (potentially sending updates to 100+ clients). Figma likely uses efficient broadcast data structures to send messages to many clients, and possibly some form of state mirroring on edge servers for geographic performance. Still, because every operation is small and the server is highly optimized, this scales well for typical use (most files have a handful of active editors, not thousands). One limitation here is offline support: since Figma’s architecture is fundamentally server-centric, fully offline collaboration (multiple people editing while disconnected) is not supported. A user can work offline alone, but true multi-master editing requires a complex CRDT approach which Figma explicitly avoided ￼. The benefit is greatly simplified infrastructure and faster performance by avoiding the overhead of CRDT metadata and merges. In summary, Figma trades off decentralization for speed and simplicity – a choice that works well given that an internet connection is available most of the time in their use case.

## Trade-offs and Limitations

Linear’s approach has the advantage of offline capability and instant UI response. It provides a great user experience – data updates feel instantaneous and you can even use the app with no internet for a while. The trade-off is increased complexity on both client and server:

    - The client must maintain a local database and logic to apply/rollback changes, handle versioning, etc. This adds development overhead (e.g. migrating the local DB schema when the app updates, dealing with IndexedDB quirks ￼ ￼, etc.).
    - The server has to keep an operation log and possibly a mechanism to deliver delta updates. Over time, that log can grow large. A naive implementation might lead to performance issues if not managed (Linear likely had to implement pruning or efficient queries by ID). It’s also an additional moving part compared to a stateless REST API.
    - Because Linear chose a simpler conflict resolution (last writer wins), there’s a risk of overwriting changes in concurrent edits. In practice this is rare, but it’s a conscious limitation – if two people edit the same issue description offline at the same time, whichever syncs last will overwrite the other. This is considered acceptable in exchange for simplicity; however, it might surprise users in those edge cases (no automatic merge like Google Docs).
    - Another trade-off is that initial load can be heavy if an account has a lot of data. Linear mitigates this with partial bootstraps and by not syncing rarely used data until needed ￼. Still, a first-time sync might involve hundreds or thousands of records, which could be slow on poor networks. A pure on-demand API would transfer less data upfront. Linear’s decision favors speed after the first load at the cost of a larger upfront payload.

Figma’s approach offers true real-time collaboration with strong consistency – it was the first design tool to allow multiple people literally editing the same canvas at once. The trade-offs and limitations include:

    - No full decentralization: You cannot have two people working truly independently and then merging automatically. A server (cloud) is required to mediate collaboration. If the server goes down or you have no internet, collaborative editing halts. This was a deliberate choice to avoid the complexity and performance cost of CRDTs in a design tool ￼, but it means Figma is not “local-first” in the way Linear is.
    - Offline support is limited: You can continue working offline in Figma, but you’re essentially working in isolation until you reconnect. When you do, if others made changes, one version’s changes will win out in each conflict. There’s no intricate merge; essentially Figma assumes offline editing is either short-lived or single-user. This is usually fine, but it’s a limitation compared to a CRDT-based system which would allow multi-user offline edits merging later. In a sense, Figma’s system is optimized for “always online” use and handles offline as a special-case, whereas Linear’s is built with offline as a first-class scenario.
    - Higher server cost/complexity: Figma’s architecture requires a lot of server infrastructure. Each document needs a dedicated process maintaining state, and updates need to be broadcast to all clients. This is more complex to scale in some ways than a simple REST API. They had to invest heavily in optimizing server performance (e.g. using Rust, efficient data structures) to achieve the fluid experience. The benefit is that this server can enforce invariants and simplify clients, but the cost is running stateful servers and ensuring reliability. It’s a trade-off of moving complexity to the backend to keep clients thin.
    - Learning curve and implementation effort: Building a system like Figma’s from scratch is non-trivial. It’s a custom solution tailored to Figma’s needs. For example, handling real-time cursor movements, drawing operations, etc., required developing a bespoke protocol. This approach might be overkill for simpler applications. As many in industry have noted, it’s often not necessary to implement full OT or CRDT for most app domains ￼. Figma’s solution is “just complex enough” to solve collaborative design editing, but reusing it wholesale for another app would be difficult. Linear’s sync engine, while still complex, deals with more standard CRUD data and might be easier to adapt to other products.

In summary, Linear’s sync engine trades some eventual consistency guarantees (no complex merges) for simplicity and offline support, whereas Figma’s engine trades offline freedom for real-time precision and consistency. Both avoid the extreme of over-engineered distributed conflict resolution and instead find a practical middle ground suited to their product requirements. The key lesson from both is that tailoring the sync approach to your app’s needs (and user behaviors) is crucial – and often a simpler approach with a central source of truth and ordered updates yields great results with far less complexity than fully generic solutions ￼.

# Implementing a Sync Engine with Rust and Postgres

Building a lightweight sync engine yourself is feasible by borrowing ideas from Linear and Figma. The goal here is a maintainable solution that a single engineer can manage, using Rust on the server (with PostgreSQL as the database) and a TypeScript client library for WebSocket communication. We’ll outline the architecture and practical steps:

## Architectural Design (Rust + Postgres Backend)

Core concept: Use Postgres as the source of truth for data, and maintain a sequence of operations (changes) that can be sent to clients. The server will accept writes (creates/updates/deletes), record them as ordered events, and push these events to clients via WebSockets for real-time sync. Each client also retains some local state (possibly a cache or partial database) to allow instantaneous updates and offline use.

Key components of the design:

    - Data Models and Tables: Design your database tables for the actual entities in your app (e.g. tasks, comments, users, etc.). Each table should have a primary key and perhaps an updated_at timestamp or a version number column to detect changes. In addition, create a “sync log” table that records changes. For example, a table sync_events with columns: id (big serial auto-increment), model (which table/model the change is for), model_id (the primary key of the row that changed), action (insert/update/delete), and maybe a data JSONB column with the new state or changed fields. Each time a write occurs, you will insert a row into sync_events representing that change.
    - Monotonic IDs for events: Using a database auto-increment ID for sync events (or a timestamp if global ordering can be guaranteed) is important. This ID establishes a total ordering of all changes in the system. As we saw with Linear’s lastSyncId, this enables efficient client catch-up and consistency ￼. Postgres’s serial or identity columns can serve this purpose. Every new event gets the next ID, so it’s trivial to ask “give me all events after ID X”.
    - Rust Server Structure: You can use an async Rust runtime (like Tokio) and an HTTP/WebSocket library (such as Warp, Axum, or Tide) – but even the standard library with a crate like hyper plus a WebSocket library (tungstenite or tokio-tungstenite) would work. The server will likely have two main parts:
    1.	An HTTP API (or GraphQL, etc.) for any initial data fetch and for clients to send updates. For simplicity, you might have a REST endpoint or GraphQL mutation that the client calls when it makes a change (e.g. “update task title”). This endpoint writes to the database (updates the task row, and inserts a new sync event within the same transaction).
    2.	A WebSocket endpoint that clients connect to for receiving real-time updates (and possibly also sending updates). Once connected, the client will subscribe to certain data changes. In a simple case, all clients in an app or in a certain org/room get the changes relevant to them. You might tag each event with a “group” or scope (like Linear’s workspace or Figma’s document room) so the server knows which connected clients should receive it.
    - State in Server: The Rust server doesn’t need to hold the entire dataset in memory (since Postgres has it), but it will keep track of connected clients and their subscriptions. For instance, maintain a map of active WebSocket connections and what org or document they belong to, so when a new sync_event is inserted, you know which sockets should get it. If you anticipate a lot of events, you might broadcast a “shoulder tap” instead of the full data (i.e., send a notification telling clients to fetch the latest changes) ￼. However, a simpler approach (used by Linear) is to push the actual data in the WebSocket message, so clients don’t need to make another round-trip for it ￼. Given our goal of minimal server load, you can choose to push just the essential fields that changed.
    - Initial Sync (Bootstrap): Provide a method for a client to get its initial state. One approach is an HTTP endpoint /bootstrap that returns all the current data the client is authorized to have. This can be heavy, so you might allow querying specific subsets (Linear’s bootstrap supports partial model lists ￼). Alternatively, you can do the initial sync over the WebSocket right after connection: e.g., client connects and sends a message “INIT {lastSyncId: 0}”, and server replies with all data or all events since 0. For efficiency, the initial load might just dump the full records (since sending potentially thousands of individual events could be slower than one bulk transfer). After the initial load, the client should store the latest sync_event.id it received – that becomes its lastSyncId.
    - Delta Sync & Recovery: Using the ordered event log, implement a way for clients to catch up if they fall behind or reconnect. For example, if a client reconnects and says “my last sync event was ID 1000”, the server can query SELECT * FROM sync_events WHERE id > 1000 AND id <= current_max_id and send those as a batch. This is exactly how Linear’s /sync/delta works ￼. This design ensures you don’t have to keep long connections alive at all times; a client that disconnects for a while can cheaply sync the diff when back. It also future-proofs against server restarts – even if the WebSocket missed some pushes, the data is still in the DB log and can be retrieved. Consider also handling the case where the gap is too large (maybe the log was truncated or the client is very out-of-date) by falling back to a full re-sync (sending full data again).
    - Postgres considerations: Postgres can handle quite a lot of writes, but writing every single change both to the main table and a log table is double-write. Use transactions so that the main data and sync log are always consistent (either both written or none). For performance, ensure the sync_events table’s ID is indexed (primary key) and perhaps partitioned or sharded by scope (e.g., have an org_id column and partition by org, so you can query “events for org X after ID Y” quickly without scanning others). If you expect a high volume of events, consider strategies to compact the log: for example, you might not need to keep events older than a certain age if all clients are assumed to have synced or if you regularly snapshot state. However, premature optimization isn’t needed if volumes are low; a single engineer can likely maintain this as long as the data stays in a few million events range.

## Sync Protocol over WebSockets (Client Integration in TypeScript)

Design a simple WebSocket protocol to handle bi-directional sync communication:

    - Connection & Authentication: The client (web or mobile) opens a WebSocket connection to the server (e.g., wss://api.yourapp.com/sync). If authentication is needed, you can use a token either in the URL or as the first message after connect. Once connected, the client will send an initial message indicating what data it wants and its last known sync state. For example: { type: "subscribe", orgId: 123, lastSyncId: 1050 }. This tells the server which “channel” or scope the client cares about (so it doesn’t get irrelevant data) and what it has already seen.
    - Server Response (Bootstrap): The server, upon receiving the subscribe message, can respond with a bootstrap payload. This could be either a full state dump or a set of recent sync events. A common approach is to send a full snapshot of each model the client needs (like Linear does with lines of ModelName={...} for each record ￼). Alternatively, if the client provided a valid lastSyncId that isn’t too old, you can send only the diff: e.g., { type: "bootstrap", events: [ ... ] , latestSyncId: 1100 }. Keep this logic simple to start: perhaps always send the full current state for the models of interest. This ensures the client is in sync and establishes a baseline lastSyncId.
    - Real-time Updates: After initialization, the WebSocket stays open for real-time events. Define a message format for updates. For instance: { type: "sync", events: [ {id: 1101, model: "Task", modelId: 42, action: "update", data: { status: "done", updated_at: "..."} } , {id: 1102, model: "Comment", modelId: 99, action: "insert", data: { ...comment fields... } } ] }. This mirrors Linear’s approach where a “sync” message contains an array of events ￼. Batching multiple events in one message improves efficiency. The client, upon receiving this, will apply each event to its local state (update the corresponding object or remove it if deleted, etc.) and update its lastSyncId to the highest received (e.g., 1102 in this case).
    - Client-side Application of Events: Implement the client-side logic in the TypeScript library to handle these events. If you have a state management system (like Redux, MobX, or even a simple in-memory cache), the library should map the incoming events to state updates. For example, on an “insert” event, add the new object to the store; on “update”, merge the changes into the existing object; on “delete”, remove it. Because this logic can be repetitive, try to generalize it – e.g., maintain a dictionary of model type to a collection in local state.
    - Sending Changes from Client: Decide if client-originated changes go through the WebSocket or a separate API. Linear chose to send writes via GraphQL/HTTP and only use the WebSocket for server-to-client pushes ￼. You could do the same for simplicity (especially if you already have a REST/GraphQL API for mutations – reusing it means you handle auth and validation in one place). Alternatively, you can allow sending events over the WebSocket as well. For instance, if a user edits a task, the TS library could send a message { type: "mutation", id: "temp-id-123", action: "update", model: "Task", modelId: 42, data: { status: "done" } }. The server upon receiving this would perform the update in the DB, create a sync event with a new ID, and then broadcast that event back out (including to the sender). Handling mutations over WS can reduce latency (one less HTTP round trip) and unify the channel, but it requires a bit more protocol design (you might need to acknowledge the mutation, etc.). For a maintainable MVP, it’s perfectly fine to do what Linear does: use a normal HTTP request to perform the change and let the resulting event come through on WS. This also avoids complex synchronization if the WS connection isn’t reliable for sending (since HTTP will ensure the write gets to the server).
    - Mobile Considerations: If you have a mobile client (say a React Native app or a native app with a TS/Javascript core), the same WebSocket library can often be used. Ensure the TS sync library is written in a platform-agnostic way (no direct DOM assumptions). If using React Native, you have WebSocket support and can use the same handshake and message logic. For pure native mobile (Swift/Kotlin), you’d implement a corresponding WS client logic, but that’s beyond TS scope. The key is the protocol: any client that speaks this JSON (or binary) protocol can participate in sync. TypeScript just makes it easier to share code between web and maybe Node/Electron environments.
    - Error Handling & Reconnects: The protocol should handle cases like the connection dropping. The TS client library can implement an auto-reconnect with exponential backoff. When reconnecting, it should again do the handshake and include the last synced ID it processed. This way the server can send any missed events while the client was offline. If the client was offline for a long time, the server might decide to ask the client to do a fresh bootstrap (if the diff is too large or log pruned). In the protocol, this could be a message like { type: "resync_required" } upon which the client knows to refetch everything via a separate route. Keeping these mechanisms simple will make the system more robust.

## Managing Client State, Batching Updates, and Optimizing Server Load

Efficient state management and batching are crucial for a smooth sync engine:

    - Local State Management: Each client maintains a local copy of the data it cares about. This could be in memory (JavaScript objects managed by a state library) or in a persistent store (IndexedDB for web, SQLite for mobile). For a lightweight approach, start with in-memory or simple storage – you can always add persistent caching later. The local state allows the client to apply incoming changes instantly and to perform optimistic updates. For example, when a user edits an item, update the local state immediately (optimistic UI), so the app feels responsive, and then rely on the sync mechanism to reconcile with the server’s truth. This is the pattern Linear uses – most interactions feel instant because they update local state first, and the server sync confirms or corrects it.
    - Batching Outgoing Updates: If the client performs many changes in a short time (say the user toggles 10 checkboxes quickly), it’s inefficient to send 10 separate network messages. Implement a simple batching or debouncing for outgoing mutations. For instance, accumulate mutations for a few milliseconds and send one batched request or message. You can also coalesce changes: if the user toggled the same item twice quickly, you really only need to send the final state. Your TS client library could offer an API like sync.updateTask(taskId, changes) that queues the change and flushes at the next tick or after, say, 50ms. However, be careful to preserve order of operations – if two different items changed, you should still send both in order (just in one payload if possible). On the server, if you receive a batch of changes, you can handle them in a single transaction (applying each and generating events).
    - Batching Incoming Updates: On the server side, try to send updates in batches when possible. As seen in Linear’s example, the server often sends an array of SyncActions together ￼. If several changes occur nearly simultaneously (or as part of one user action), group them into one WebSocket message. This reduces overhead from multiple small messages and ensures the client applies a consistent set together. For example, when a new comment is added, Linear sends the new comment and the updated issue (with maybe an updated comment count or last modified timestamp) in one go ￼. You can adopt the same pattern: have your server collate all events generated in one transaction and emit them together.
    - Reducing Server Load: One strategy to minimize load is to limit what each client subscribes to. If your app is multi-tenant or has distinct documents, don’t send every update to every client. Use subscription scopes (like org ID, document ID, or user-specific filters). For example, if you have multiple projects, the client could subscribe only to the projects it has open. This way, when a change happens in Project A, the server doesn’t waste effort sending it to users who only care about Project B. Designing your sync with this selective sync in mind can drastically cut down on unnecessary processing and bandwidth. It’s similar to Figma’s approach of isolating documents and Linear’s approach of isolating workspaces.
    - Another optimization is to offload some work to the database. Postgres has a feature called LISTEN/NOTIFY, which can act like a trigger when certain tables change. In a simple setup, you might not need it, but if you had multiple server instances, NOTIFY could let all instances know about new events. A single-engineer system might just run one server process, in which case you can skip that and simply directly broadcast from the process that handled the DB write.
    - Throughput considerations: If your sync engine usage grows (say lots of clients and events), monitor how many events per second you handle. Rust is quite capable of handling thousands of websocket messages per second on modest hardware, especially with Tokio async. The main load on the server is likely the database writes. Use batch inserts where possible (inserting multiple events in one SQL statement for a batch), and make sure to use indexes for queries. If things get heavy, you could introduce caching layers or in-memory buffers, but to keep it maintainable, try to rely on the strengths of Postgres (its ability to handle concurrent writes and indexed reads).
    - Client Memory Management: If the dataset is large, clients should prune or virtualize data not in use. For instance, Linear caches a lot in IndexedDB, but likely only keeps active data in memory. If your app has thousands of items, don’t render or hold all in React state at once – use techniques like pagination or on-demand loading into view, and still keep them in local database for quick access when needed. This isn’t directly about sync, but it affects perceived performance.

In summary, batching and filtering are your friends: batch network operations to reduce chatter, and filter subscriptions so each client and server only deals with what they must. This yields a system where even a single modest server can handle many clients, because it’s not doing wasted work.

## Ensuring Consistency Across Clients (Without Over-Engineering Conflicts)

Keeping multiple clients consistent sounds daunting, but with a centralized design it becomes much simpler:

    - Single Source of Truth: Always have the server (and the database) be the final arbiter of truth. Even if clients do optimistic updates or have local caches, the server’s version wins eventually. This prevents divergent states. Any client-originated change should go through the server (either via HTTP or WS) and result in an official event. Design your client to gracefully accept that its local optimistic changes might be overwritten by the server’s response. For example, if two people edit the same field, one client’s change may get overwritten – the UI should reflect the final state without blowing up (maybe show a subtle warning, or simply update to the new value).
    - Last-Writer-Wins and Timestamp Ordering: The simplest resolution strategy is last-writer-wins. That effectively means whichever operation is applied last in the global order is what everyone ends up with. This is what our sync log inherently does – it orders every event. You typically won’t implement explicit “conflict resolution” logic in code; rather, by applying events in the order they were generated, you’ve implicitly resolved conflicts by sequencing. One thing to be careful of: if your system allows offline edits, two conflicting edits might both reach the server later. They’ll still be ordered by arrival (or by an explicit timestamp each carried). You might choose to use timestamps to order them if that matters (though clocks can skew). Often, it’s fine to just use arrival order or an incrementing sequence as the truth.
    - Avoiding Complex Merge Algorithms: Don’t attempt to write a general OT or CRDT layer for all your data – that’s the over-engineering we want to avoid. As we discussed, most apps don’t need that, and simpler strategies work well ￼. Instead, handle conflicts in a pragmatic way:
    - If two different fields are updated concurrently, both changes can be applied without conflict (because they don’t overwrite each other). E.g., User A changes a task’s title while User B changes its due date – when events arrive, the order doesn’t matter; the final state has both the new title and new due date. Your sync events can even be field-scoped (only include changed fields) so that merging is naturally handled by applying both.
    - If the same field is updated concurrently (e.g., both A and B change the title), then whichever update is applied second will override the first. This is fine; the first update is essentially lost. In a task management context, such exact conflicts are rare. If it happens, it’s not the end of the world – the user who got overridden might notice and reapply their change if needed. Often, users communicate to avoid this, or the app UI could lock the field when someone is editing (if you want to go one step further).
    - For operations like deletions, you might enforce some logic like: if an item was deleted by one user, and another user later tries to edit it, the edit will fail because the item no longer exists. In such cases, you’d want your server to check that the item is still active before applying an update. If not, it can reject the operation or ignore it. These are simple integrity checks rather than conflict “merges”.
    - Version Checking (optional): A lightweight addition is to use a version number or updated_at on each record. When a client sends an update, include the version it last saw. The server can compare – if the record’s current version doesn’t match, it means the record was modified by someone else in the meantime. You can then decide to reject the update (forcing the client to refresh) or accept it anyway (overwriting the other change). For simplicity, you might skip this at first, but it can be useful for certain high-stakes data where you prefer to avoid silent overrides. Even with version checking, you may still choose to accept the latest write (just logging that a conflict happened).
    - Testing with Real Scenarios: As a single engineer maintaining this, you’ll want to test scenarios like: two browsers open as two different users editing the same data. Ensure that both see consistent outcomes. With the design above, you should see that both end up with the same final state after each operation sequence. Also test offline/online transitions: make some changes offline (i.e., simulate by not sending them to server immediately), have another client make changes in the meantime, then sync. Verify that your delta logic brings the offline client up to date and then applies its changes. Typically, the offline client’s changes will apply after the others, effectively making it the last writer for those fields it changed. That means it might override some data. If that’s acceptable, great. If not, you’d need a more complex policy.
    - User-Facing Handling: Because we’re not over-engineering, in the rare case of a true conflict, it might be fine to simply let one win. If you want, you can add minimal user-facing cues. For example, if an update fails because the item was modified or deleted, show a message: “This item was updated by someone else, please refresh.” This is a simple way out instead of trying to auto-merge everything. Many collaborative apps do something similar for conflicting edits that they can’t reconcile easily.

By following these strategies, consistency will be eventually achieved across all clients without building a complex distributed conflict resolver. The central server and ordered event log ensure that all clients process changes in the same order ￼. As long as your apply logic is deterministic, two clients given the same starting state and the same sequence of events will end up with the same final state. This is the core principle behind Linear’s and Figma’s engines. And because conflicts are rare and the system is fast at propagating updates, users will seldom encounter issues. One analysis of real-world collaborative apps noted that developers often find their “naive” sync solutions work fine and that not overcomplicating things is actually a common trait of successful systems ￼.

## Final Thoughts

With the above design, you have a relatively lightweight sync engine: a Postgres-backed event log, a Rust WebSocket server to ferry updates, and a TypeScript client to apply them. Such a system benefits from decades-old, solid technology (SQL databases, socket communication) with just a thin layer of custom logic. It is maintainable by a single engineer – most complexity (data storage, networking) is handled by Postgres and Rust’s libraries. By focusing on simple conflict resolution (mostly via ordering and last-write-wins), you avoid the massive complexity of algorithms like OT or CRDT while still delivering a robust real-time collaboration experience. This approach follows the lead of tools like Linear and Figma, which demonstrated that carefully designed pragmatic sync engines can achieve excellent UX without needing to reinvent theoretical models from academic research. Your implementation can remain simple, reliable, and efficient – very much a “Toyota” rather than a Formula 1, to borrow an analogy ￼ ￼ – which is exactly what you want for long-term maintainability.

Sources:

    - Linear Sync Engine architecture and offline-first design ￼ ￼
    - Figma’s multiplayer architecture and decision to avoid CRDT complexity ￼ ￼
    - Strategies for simple conflict resolution in collaborative apps ￼ ￼
    - Discussion of OT/CRDT overkill vs. last-writer-wins simplicity ￼
    - PowerSync (Postgres<>SQLite sync) insight on operation logs and partial sync ￼ (conceptual inspiration)
